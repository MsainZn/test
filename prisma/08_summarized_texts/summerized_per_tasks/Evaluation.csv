Title,DOI,Responsible,Model,Surgery/Modality,Prediction Type,Summary,Limitations,Evaluation,Dataset (link),Size/Annotations,Ethnicities,Age range,Sex,Acquisition Centers
Breast retraction assessment: an objective evaluation of cosmetic results of patients treated conservatively for breast cancer,https://doi.org/10.1016/0360-3016(85)90190-7,Helena,Traditional,Lumpectomy,Objective Metric, Symmetry-based method: BRA - evaluates the vertical and horizontal displacement of the nipples by calculating their distance to the sternal notch,"Requires keypoint annotations; Sensitive to small alterations to keypoints, particularly in the sternal notch, which may not be visible in frontal images",,Private (no link),56 patients,N/R,N/R,F,N/R
Cosmetic evaluation of breast conserving treatment for mammary cancer. 1. proposal of a quantitative scoring system,https://doi.org/10.1016/0167-8140(89)90016-9,Helena,Traditional,Lumpectomy,Objective Metric,Symmetry-based method: LBC - evaluates the displacement between the lowest contour of the breast; UNR - evaluates the vertical displacement of the nipples by calculating their relative distance (UNR),Requires keypoint annotations,,Private (no link),142 patients,N/R,N/R,F,N/R
Breast compliance: a new method for evaluation of cosmetic outcome after conservative treatment of early breast cancer,https://doi.org/10.1007/BF01806355,Helena,Traditional,Lumpectomy,Objective Metric,Symmetry-based method: BCE - evaluates the vertical displacement of the nipples by calculating their distance to the lowest point of each breast,Requires keypoint annotations,,Private (no link),151 women,N/R,N/R,F,Breast Unit at Guy's Hospital
Towards an intelligent medical system for the aesthetic evaluation of breast cancer conservative treatment,https://doi.org/10.1016/j.artmed.2007.02.007,Helena,Traditional,Lumpectomy,Objective Metric,"BAD, BOD, etc",Requires keypoint annotations,,Available upon request,113 patients (PORTO),white (portuguese),N/R,F,1 clinical center
An Artificial Intelligence Approach to the Assessment of Abnormal Lid Position,https://doi.org/10.1097/GOX.0000000000003089,Helena,Traditional,Ptosis,Objective Metric,"Obtains two objective metrics - Vertical Palpebral Aperture Measure, Interpupillary Distance - based on face landmarks.",,,Private (no link),32 pre/post-op pairs,N/R,N/R,N/R,Google search (multi source)
Three-Dimensional Analysis of Nasal Symmetry following Primary Correction of Unilateral Cleft Lip Nasal Deformity,https://doi.org/10.1597/16-073,Helena,Traditional,Rhinoplasty,Objective Metric,Propose RMSD - root mean squared deviation between native and reflected nose surfaces for the evaluation of asymmetry in rhinoplasty for the reconstruction of nasal deformities due to cleft lip.,,,Private (no link),76 patients,N/R,N/R,N/R,N/R
Objective and Interpretable Breast Cosmesis Evaluation with Attention Guided Denoising Diffusion Anomaly Detection Model,https://doi.org/10.1016/j.ijrobp.2024.07.1427,Helena,DDPM,Mastectomy,Objective Metric,"Uses self-supervised ViT to identify relevant image regions (breasts), generating a mask which is then used to condition a DDPM to generate breast images while preserving the background and manupulating the breasts within the mask according to the probability distribution of the data (mostly composed of Excellent/Good images). Then, it uses the DDPM's reconstruction error as the anomaly score. The images generated by DDPM can be used as explanations and add interpretability","The images generated by DDPMs have some variability, which may affect the anomaly score",Evaluates correlation between anomaly score and aesthetic evaluation labels; Cohen Kappa to evaluate agreement with experts; compares results against BCCT.core,Private (no link),"1537 images, 300 with aesthetic scores",N/R,N/R,F,Yonsei Cancer Center
Feasibility of anomaly score detected with deep learning in irradiated breast cancer patients with reconstruction,https://doi.org/10.1038/s41746-022-00671-0,Helena,GAN,Mastectomy,Objective Metric,"Uses a GAN for unsupervised anomaly detection, calculating an anomaly score as an objective metric for aesthetic evaluation. The GAN is first trained on 3D images obtained based on CT scans, then an encoder is trained to reconstruct the original image. The anomaly score is the reconstruction error + error between features extracted by discriminator",Lack of interpretability. It is unclear what the anomaly score specifically evaluates in the images (particularly the discriminator feature representations).,N/A (Evaluates surgeries rather than anomaly score),Available upon request,train: 251 CT images of patients; test: 61 CT images of patients,N/R,32 <=45yo; 29 >45yo,F,Seoul National University Bundang Hospital
Evaluation of cosmetic outcomes in breast reconstruction patients undergoing radiotherapy using an anomaly generative adversarial network model,https://doi.org/10.1038/s41598-024-66959-1,Helena,GAN,Mastectomy,Objective Metric,Same as above on a new dataset,same as above,same as above,Available upon request,82 CT + postop images of patients,N/R,"[29,62], mean=46",F,Seoul National University Bundang Hospital
Applying artificial intelligence to assess the impact of orthognathic treatment on facial attractiveness and estimated age,https://doi.org/10.1016/j.ijom.2018.07.010,Helena,VGG-16,orthognatic surgery,Atteractiveness score (1-100),"Trains CNN to predict facial attractiveness in a scale from 1 to 100 on images and ratings from a dating site, and applies it to evaluate pre- and post- orthognatic images, verifying increases in attractiveness between pre- and post- images",,,Private (no link),2164 pre/post pairs of 146 patients,N/R,N/R,N/R,Clinic of Orthodontics and Paediatric Dentistr
Turning subjective into objective: the bcct.core software for evaluation of cosmetic results in breast cancer conservative treatment,https://doi.org/10.1016/j.breast.2007.05.002,Fábio,Traditional,Lumpectomy,Objective Metric,"converts subjective cosmetic assessments into quantitative scores. The BCCT.core software requires the user to select scale and reference points (e.g., sternal notch and breast contour) from digital photographs to compute symmetry measures— nipple displacement, scar , color differences—and then compares these metrics against expert consensus using kappa statistics.","manual selection of keypoinst. Agreement with expert consensus can vary, with overall moderate interobserver consistency that improves when intermediate classes are merged.",Reported interobserver agreement (using kappa statistics) was fair to moderate overall (kappa ≈ 0.34–0.53) but improved to substantial levels (kappa ≈ 0.60–0.73) in cases with first-round consensus.,Private (no link),TSIO - 30 post-operative images with landmark annotations and aesthetic scores,N/R,N/R,F,N/R
A fully automatic framework for evaluating cosmetic results of breast conserving therapy,https://doi.org/10.1016/j.mlwa.2022.100430,Fábio," DL fully automatic pipeline using YOLO‑v3 for detecting breasts, areolas, and nipples; separate Ensemble Regression Trees for landmark detection; and machine predictors (Lasso Regression, kSVC with RBF kernel, OrdinalNet) for classification",Lumpectomy,Objective Metric," Processes frontal digital photographs by first standardizing (color adjustment and cropping) the image, then automatically detecting breast components and extracting 298 hand‐crafted cosmesis features based on landmark positions.
Predicts cosmetic outcomes into 4 classes and also includes sub classifications for specific features (breast size, shape, nipple appearance, scar appearance, skin color, and global score) without human supervision.","• Performance depends on robust detection and landmark extraction; errors in small regions (areolas, nipples) can affect feature quality.
• LOTO cross-validation results indicate potential bias when applied across different trials.","Evaluated using both 10‑fold and Leave-One-Trial-Out (LOTO) cross-validation.
For component detection, metrics such as mAP (>0.85) and F1 scores are reported.
Landmark detection is measured by normalized Euclidean error (e.g., errors below ~2% of the half inter‑pit distance in 10‑fold CV).
Overall 4‑way classification accuracy is used to assess the full pipeline, with results comparable to BCCT.core.",Public,3762 frontal + 4316 lateral post-op images with aesthetic scores and landmark annotations,N/R,N/R,F,multi-trial (potentially multi-center)
kOBCS©: a novel software calculator program of the Objective Breast Cosmesis Scale (OBCS),https://doi.org/10.1007/s12282-019-01006-w,Fábio,Traditional (calculator for OBCS),Lumpectomy,Objective Metric,"Software created specifically for ease of calculation of OBCS
digital photos could be directly imported into the program and the dimensions required for calculations are measured by placing the different angel points of two automatically generated triangles at the suprasternal notch (SSN), both nipples, and at the point of intersection between the projection of the lateral and the lower breast contours bilaterally","Manual intervention required. Only takes into account symmetry (no scar, pigmentation etc...). 
Binary classification, doesnt use full range of Harris scale",Evaluates cosmosis according to OBCS,Private (no link),30 post-op images,N/R,N/R,F,N/R
Measurement of adverse cosmesis in breast cancer: A deep learning approach,https://doi.org/10.1016/j.eswa.2023.122209,Fábio,"ResNet‑18 + SVM (3 one‑vs‑rest cls.)
", solution is agnostic but paper focuses on Lumpectomy,Objective Metric,"Pre‑processes frontal photographs to obtain rough breast ROIs, extracts deep features with a pre‑trained ResNet‑18, and trains three SVM classifiers (RAPID, OPAR, PORTO) to flag adverse cosmesis after BCS/RT. Validated internally and on three independent trial datasets, achieving AUC 0.78–0.94 and beating BCCT.core","Minimal manual intervention required (ROI selection)
Binary classification, doesnt use full range of Harris scale
","The trained classifiers/models were evaluated
through 5 experiments: holdout validation, treatment-arm specific validation in RAPID, external validation
(included datasets from the remaining 3 trials), assessment of the effect of preprocessing, and comparison with
BCCT.core software.","6 datasets (5 from BCS/RT-related clinical trials); RAPID and OPAR are private, the others are public
Training subsets (80% of patients) from 3 datasets (RAPID, OPAR, and PORTO)",PORTO (113) + Guo et al (3762) + RAPID (2135) + OPAR (281) with aesthetic outcomes,N/R,RAPID: >=40yo; OPAR: >50yo,F,Multiple datasets
The use of a breast symmetry index for objective evaluation of breast cosmesis,https://doi.org/10.1016/j.breast.2007.01.013,Fábio,Traditional,Lumpectomy,Objective Metric,"BAT software analyses one frontal + one lateral photo; user marks jugulum, nipples & breast contour; program outputs BSI‑frontal, BSI‑side and BSI‑total (% difference & factor). Tested on 27 post‑BCT patients and 5 schematic drawings","Binary classification, doesnt use full range of Harris scale
Only takes into account symmetry (no scar, pigmentation etc...). 
Requires manual input",Software vs hand‑measure r = 0.998; inter‑observer r = 0.90; repeated‑measure ICC ≈ 0.97; BSI‑frontal correlates with Harris scale r = 0.769; BSI < 30 % indicates good cosmesis.,27 BCT patient photos (frontal + side) – not public,frontal and side post-op images of 27 patients,N/R,N/R,F,N/R
A Deep Learning Approach for Assessment of Breast Aesthetic Score and Keypoint Localization,https://doi.org/10.1109/COMPSAC61105.2024.00234,Fábio,EfficientNet-B0 (4-class Harvard score) + DeepLabV3-ResNet50 (13 + 2 keypoints),Lumpectomy,Objective Metric,"5 854 photos with Harvard scores and 4 274 photos with 28 keypoints (SNUBH + public CTR). End-to-end pipeline: preprocessing → EfficientNet for score, DeepLabV3 for 15 keypoints; no manual ROI needed.",Accuracy only 55 %; keypoint errors when armpits occluded by clothes; frontal images only,"Score model: accuracy 0.554, MAE 0.495, Kendall τ 0.544; keypoint model: normalized L2 error 0.070 ± 0.194; DeepLabV3 out-performs FCN.",SNUBH and public datasets from clinical trials,Guo et al (3762) + private (2092) with aesthetic scores and partially annotated with landmarks,N/R,N/R,F,Seoul National University Bundang Hospital
A Novel Artificial Intelligence Model for Symmetry Evaluation in Breast Cancer Patients,https://doi.org/10.1007/s00266-023-03554-1,Fábio,Pyolo8 (YOLOv8 landmark detector) + BAS-Calc symmetry index,agnostic,Objective Metric,"Landmark detection model Trained on 622 annotated photos; detects suprasternal notch, nipples & breast borders with YOLOv8, pipes to BAS-Calc to output a symmetry index – fully automated, no manual clicks. Tested on 81 reconstruction photos; 399/405 landmarks detected (98.5%).",Symmetry only. Misses suprasternal notch in adipose patients,Landmark detection 98.5 %; symmetry score strongly correlated with surgeons (Pearson 0.93-0.95; Spearman 0.90-0.91); Cohen κ 0.79-0.87; mean compute time 0.92 s vs 14 s human.,622 training photos + 81 test photos from Albacete Univ. Hospital; not publicly released.,703 post-op images,N/R,mean=51,F,Albacete Univ. Hospital
Objective assessment of aesthetic outcomes of breast cancer treatment: Measuring ptosis from clinical photographs,https://doi.org/10.1016/j.compbiomed.2005.10.007,Fábio,Traditional,Mastectomy,Objective Metric,"52 patients (pre-op) photographed in 5 views. 
Observers manually mark sternal notch, nipple, inframammary fold terminus & lowest point; software computes two ratios → ptosis grade. 
Regression aligns ratios to 4-level subjective scale.",Manual landmarking; no automatio; measures ignore .,"Intra-observer κ 0.52-0.84; inter-observer κ 0.23-0.49. Measure #1 R² 0.48, Measure #2 R² 0.58 vs mean surgeon ratings. Landmark variability < 0.5 subjective grade; novices needed ≈46 s/photo.","52 women, 5 standard views each; private.",5 views of 52 post-op patient breasts with ptosis rating and landmarks,N/R,>= 21yo,F,N/R
Towards complementary explanations using deep neural networks,https://doi.org/10.1007/978-3-030-02628-8_15,Pedro,DNN,Breast Cancer and melanoma,Objective Metric and explanation generation,Uses a DNN to generate complementary explanation for its predictions for better interpretability,Subjective explanation results ,"ROC and PR for predictions and Correctness, Completeness, and Compactness for the explanation","Private (no link) for the breast cancer dataset
Also tested on PH2",PORTO + TSIO,...,...,...,...
How to produce complementary explanations using an ensemble model,https://doi.org/10.1109/IJCNN.2019.8852409,Pedro,"Ensemble (scorecard, Random Forest, DNN)",Lumpectomy evaluation and melanoma detection,Objective Metric + Explanation,"Ensemble model to generate explanations for predictions, improving interpretability while maintaining high accuracy.",Effectiveness depends on the semantic meaningfulness of input features.,"ROC and PR for predictions and Correctness, Completeness, and Compactness for the explanation","Private (no link) for the breast cancer dataset
Also tested on PH2",PORTO + TSIO,...,...,...,...
An Automatic Framework for Nasal Esthetic Assessment by ResNet Convolutional Neural Network,https://doi.org/10.1007/s10278-024-00973-7,Pedro,R-CNN(VGG16 backbone) for nasal base detection + ResNet for landmark coordinates prediction + CNN-MLP for symmetry assessment,Rhinoplasty,Objective Metric,Framework to assess nasal base simmetry in rhinoplasty using landmark detection and a combined CNN-MLP model to predict symmetry scores,Performance may be decreased with imprecise bounding box location or diverse nasal morphologies (for example multi-ethnic datasets). This method also requires high-quality annotated training data.,"MAE, MAPE and NME for landmark coordinate prediction and MAE, MAPE, RMSE and Pearson Correlation for symmetry scores prediction",Private (no link) ,438 pre/post-op pairs with landmarks + 100 pairs (MERD),multi-ethnicity in MERD,"[13, 75], mean=30",370F + 68M; 74F + 26M in MERD,University of Tehran + various sources in MERD
"A Novel, Deep Learning-Based, Automatic Photometric Analysis Software for Breast Aesthetic Scoring",https://doi.org/10.1055/a-2190-5781,Pedro,DenseNet-264,Breast Cancer,Objective metric,Software (that uses DenseNet-264) for automated evaluation of breast aesthetics from 2D frontal images. Detects landmarks and uses them to calculate assymetry indices.,Uses only 2D frontal imaged; requires manual input of sternal notch-to-nipple or nipple-to-nipple distances for precise scaling; innacurate for ptotic breasts (underestimates inframammary fold distance),"Mean, Paired t-test, Upper limit of agrreement, lower limit of agreement",Private (no link),100 patient images annotated with landmarks,N/R,"[28,71] - mean=48",F,Seoul National University Bundang Hospital 
Use of Convolutional Neural Networks to Evaluate Auricular Reconstruction Outcomes for Microtia,https://doi.org/10.1002/lary.30499,Pedro,DenseNet-based CNN,Auricular reconstruction for microtia,Surgery Classification,"CNN to asses aesthetic outcomes of microtia reconstruction using 2D photographs in terms of  how ""normal"" they look. Note: predicts whether patient has been through respective surgery

","Data from internet images, potentially lacking clinical standardization; relies on 2D images only","Accuracy and AUC (on the segmentation part).Accuracy, precision, sensitivity and specificity (on the classification part)",Private (no link),1115 images with aesthetic scores,N/R,N/R,N/R,N/R
Evaluation of reconstructed auricles by convolutional neural networks,https://doi.org/10.1016/j.bjps.2022.01.037,Pedro,ResNet-50,Auricular reconstruction for microtia,Surgery Classification,ResNet to evaluate reconstructed auricles (normal or abnormal). Also used saliency maps to identify key anatomical features. Note: predicts whether patient has been through respective surgery,Limited dataset and from various sources,"Accuracy, precision, Recall, F1-score",Private (no link),400 images with aesthetic scores,N/R,N/R,N/R,multi-source (collected from public articles)
Facial attractiveness of cleft patients: a direct comparison between artificial-intelligence-based scoring and conventional rater groups,https://doi.org/10.1093/ejo/cjz007,Pedro,VGG-16,"Lip repair, soft palate repairs, alveolar bone grafting, orthognatic surgery and rhinoplasty",Objective metric,Evaluate facial attrativeness using CNN,"Struggled to detect cleft-specific features that influenced human rating, suggesting it needs more cleft-specific training data","Mean, Median, Minimum, Maximum, Imprecision",Private (no link),30 frontal and profile images,N/R,mean: 21yo,15F+15M,1 institution
Utilization of Machine Learning for the Objective Assessment of Rhinoplasty Outcomes,https://doi.org/10.1109/ACCESS.2023.3270438,Pedro,AutoGluon AutoML (Machine Learning),Rhinoplasty,Objective metric,Used synthetic 3D facial models with simulated nasal deformities to train ML models for objective rhinoplasty outcome assessment,Trained on synthetic data(not of real patients). Needs more clinical raters for better generalization,Accuracy,Private (no link),synthetic images,various ethnicities,"[23, 59]",13F+11M,synthetic
Craniofacial and Cervical Aesthetic Surgical Outcome Analysis Using Visual Similarity Metrics,https://doi.org/10.1007/978-981-99-7093-3_34,Pedro,"quantitative image similarity metrics (FSI, FPI, EDFL, PH)",Craniofacial and cervical,Objective metric,"Used four metrics: Facial Symmetry Index (FSI), Facial Proportion Index (FPI), Euclidean Distance of Facial Landmarks (EDFL), and Perceptual Hashing (PH) for evaluating aesthetic outcomes of surgery","Uses only 2D image, and not evaluated on large dataset",Accuracy,Private (no link),,,,,
Applied Deep Learning in Plastic Surgery: Classifying Rhinoplasty With a Mobile App,https://doi.org/10.1097/scs.0000000000005905,Helena,,Rhinoplasty,Deep Learning,Predicts whether face image has gone through rhinoplasty,,,Public data from RealSelf dataset: https://www.realself.com/,"22,686 images",N/R,N/R,N/R,multi-source (collected from public websites)
,,,,,,,,,,,,,,
Quantification of facial symmetry in orthognathic surgery: A novel approach integrating 3D contour maps and hyper-dimensional computing,https://doi.org/10.1016/j.compbiomed.2024.109189,,,,,,,,,,,,,
A 3D low-cost solution for the aesthetic evaluation of breast cancer conservative treatment,https://doi.org/10.1080/21681163.2013.858403,,,,,,,,,,,,,
Analysis of surgical outcome after upper eyelid surgery by computer vision algorithm using face and facial landmark detection,https://doi.org/10.1007/s00417-021-05219-8,Helena,,,Objective Metric,"Measures Vertical Palpebral Aperture, Area of the eye opening and  Average brow height. Ensemble of regression trees (DLIB-ML toolkit)",,,,55 patients,N/R,"[36, 76], mean=56",45F + 10M,Yeditepe University Hospital
An Artificial Intelligence-Based Cosmesis Evaluation for Temporomandibular Joint Reconstruction,https://doi.org/10.1002/lary.30239,Helena,,,Objective Metric,Mandibular Asymmetry Value - flips the contour of the lower jaw of the left side onto the right one and measures the displacement between the kepoints of the flipped left side and right side.,,,,,,,,
Personalized quantification of facial normality: a machine learning approach,https://doi.org/10.1038/s41598-020-78180-x,Helena,StyleGAN + MLP,Facial Surgeries,Deep Learning,"Uses a pretrained StyleGAN for anomaly detection -> obtains a latent representation of a normalized post-surgery image (without the anomaly) and compares it to the original using image similarity metrics (e.g., SSIM, LPIPS). Then, it trains an MLP based on the anomaly scores to obtain the final score predicion",,,,,,,,
,,,,,,,,,,,,,,
Facial Beauty Prediction,,,,,,,,,,,,,,
Regression Guided by Relative Ranking Using Convolutional Neural Network (RCNN) for Facial Beauty Prediction,https://doi.org/10.1109/TAFFC.2019.2933523,Fábio,R³CNN – ResNeXt‑50 backbone CNN regression branch + Siamese ranking branch; assemble (regression + LSEP pairwise) loss; cascaded fine‑tuning,Facial beauty (non‑surgical aesthetics),Objective Metric (using pairwise ranking),"CNN that integrates pairwise‑ranking guidance into a regression branch for facial beauty prediction. Key ideas: offline hard‑pair sampling, an assemble loss combining regression + ranking, and cascaded fine‑tuning. Achieves state‑of‑the‑art on SCUT‑FBP and the new SCUT‑FBP5500 dataset.","demographics limited (ages 15–60, Asian/Caucasian); Siamese ranking branch increases training cost","5‑fold CV on both datasets. Best scores: PC = 0.95 on SCUT‑FBP, PC = 0.914 / MAE = 0.212 / RMSE = 0.28 on SCUT‑FBP5500; extensive ablations on backbones, loss types, and sampling strategies.",SCUT‑FBP ( http://www.hcii-lab.net/data/SCUT-FBP/EN/introduce.html ); SCUT‑FBP5500 (https://github.com/HCIILAB/SCUT-FBP5500-Database-Release),,,,,
A Computational Approach to Relative Aesthetics,https://doi.org/10.1109/ICPR.2016.7900003,Fábio,"Siamese deep CNN: two weight‑shared branches, each taking a 224 × 224 warped whole image and one random 224 × 224 local patch (concatenated). Each branch → 5 conv layers → FC‑1000 → FC‑256 (50 % dropout). The 512‑D embeddings are subtracted and passed through two FC layers and a scalar head.",Generic photographic aesthetics (multi‑category AVA images),Pairwise relative ranking of subjective aesthetic preference,"Introduces the Relative Aesthetics task: given two photos from the same semantic category, pick the more beautiful one. Curates ≈ 43 000 high‑confidence pairs from the AVA dataset (rating gap ≥ 1, low rater variance). The Siamese network learns to score pairs directly and surpasses RAPID aesthetics classifier on ranking and binary tasks.","Uses only AVA; limited content diversity.
Ignores pairs with small rating gaps, so fine‑grained judgments are lost.
Single random patch may miss salient regions; global 224×224 warp distorts composition",On held‑out 20 k test pairs: 70.5 % ranking accuracy. On the standard 7 670‑pair AVA test split: 76.8 %. For binary aesthetics (good/poor) without retraining: 71.6 % accuracy vs 69.2 % for RAPID.,Relative Aesthetics pair set derived from AVA (https://github.com/imfing/ava_downloader) (≈ 43 k pairs); authors promised released but I couldbt find it.,,,,,
R2-ResNeXt: A ResNeXt-Based Regression Model with Relative Ranking for Facial Beauty Prediction,https://doi.org/10.1109/ICPR.2018.8545164,Fábio,"Siamese net: Backbone ResNeXt‑50 shared by (i) two identical regression subnetworks (predict 1‑to‑5 beauty score) and (ii) a Siamese ranking subnetwork (element‑wise subtraction → FC) that predicts pairwise order. Training with aggregated loss = regression + W × hinge‑margin ranking, W = 0.1 but other values were teseted",Facial beauty (non‑surgical aesthetics),Objective Metric (using pairwise ranking),"Integrates relative ranking into a regression CNN so the auxiliary Siamese branch regularises the score‑regression branch. Constructs 100 k image pairs from SCUT‑FBP via data‑augmentation and score‑difference ≤ 1. Outperforms baseline ResNeXt‑50 and prior FBP methods, showing ranking guidance boosts score prediction.","Uses only 500 Asian‑female SCUT‑FBP faces → limited demographics.
","5‑fold CV on SCUT‑FBP: PC = 0.8957, MAE = 0.2416, RMSE = 0.3046 (avg). Baseline ResNeXt‑50: PC 0.8733 / MAE 0.2585 / RMSE 0.3404. Ablations versus AlexNet & ResNet‑18 confirm depth and ranking boost. ","SCUT‑FBP (500 images, 75 ratings each). http://www.hcii‑lab.net/scut‑fbp/",,,,,
Anchor-Net: Distance-Based Self-Supervised Learning Model for Facial Beauty Prediction,https://doi.org/10.1109/ACCESS.2024.3394870,Fábio,"two identical ResNet‑50 backbones (pre‑trained on VGGFace2) process a target face and an anchor face. Each branch → global‑average‑pool → 512‑D vector; the two vectors are concatenated and passed through FC‑1024 → FC‑256 (ReLU + 50 % dropout) → FC‑1, outputting the predicted beauty‑score difference Δŷ. Training loss = L1 ",Facial beauty (non‑surgical aesthetics),Objective Metric (using pairwise ranking between image and anchor),"Proposes a comparison‑based FBP framework: instead of predicting an absolute score, Anchor‑Net learns to predict how much more (or less) attractive a face is than a reference anchor image. Anchors are chosen to share race & gender with the target and to have low base‑model error; an anchor ensemble (≈20) stabilises predictions. On SCUT‑FBP5500 the approach surpasses previous CNN, ensemble and ranking methods on PC, MAE and RMSE.","Needs a curated anchor pool matched by race/gender
Ensemble of 20 anchors increases inference time and GPU memory.
Evaluated only on SCUT‑FBP5500 (Asian/Caucasian, ages 15–60); generalisation to other populations or in‑the‑wild photos untested.","SCUT‑FBP5500 6 : 4 split: PC = 0.0021, MAE = 0.0055, RMSE = 0.0065 (scores normalised to 0–1). 5‑fold CV (ensemble, race/gender + error sampling): average PC = 0.0034, MAE = 0.0155, RMSE = 0.0135 
. Despite the tiny values (due to score normalisation), Anchor‑Net outperforms R²‑ResNeXt, R³CNN and recent ensemble losses on all three metrics 

",SCUT‑FBP5500 ( https://github.com/HCIILAB/SCUT‑FBP5500‑Database‑Release ),,,,,
Is facial beauty in the eyes? A multi-method approach to interpreting facial beauty prediction in machine learning models,https://doi.org/10.1007/s44163-025-00226-8,Fábio,"VGGFace2‑pre‑trained ResNet‑50 with frozen stages 1‑4 and a 3‑layer MLP head (GAP → BN + Dropout 0.5 → FC‑1024 + PReLU → BN + Dropout 0.5 → FC‑C where C = #rating bins). Trained with Categorical Cross‑Entropy on full rating distributions.
Two variants fine‑tuned on individual facial regions (eyes, cheeks, nose, lips, eyebrows, chin) for feature‑specific models.",Facial beauty (non‑surgical aesthetics),Objective beauty‑score distribution (1–5 or 1–10) → mean score regression,"Focuses on interpretability of FBP CNNs rather than new architecture. Proposes a multi‑method framework: (1) Permutation Feature Importance (PFI) that swaps one facial region across the test set; (2) Region Attribution (RA)—aggregates XRAI saliency within landmark‑defined regions; (3) Individual Feature Prediction (IFP)—separate CNN for each region. Experiments on SCUT‑FBP5500 and MEBeauty show eyes/cheeks rank highest by PFI, nose tops RA; feature‑specific CNNs confirm cheeks, eyebrows and nose carry most predictive power.","Both datasets cover mainly Asian/Caucasian faces;
Landmark‑based cropping may fail on large pose or occlusion.","SCUT‑FBP5500 MAE = 0.187, RMSE = 0.247, PC = 0.931; MEBeauty MAE = 0.653, RMSE = 0.849, PC = 0.770. PFI shows swapping eyes increases MAE to 0.496 (SCUT) / 1.014 (MEBeauty); RA scores rank nose highest (mean ≈ 0.93 / 0.89). IFP models trained only on cheeks, eyebrows or nose reach MAE ≈ 0.404–0.457 (SCUT) and 0.963–1.064 (MEBeauty), confirming multi‑region synergy.","SCUT‑FBP5500 ( https://github.com/HCIILAB/SCUT‑FBP5500‑Database‑Release)
 MEBeauty ( https://github.com/fbplab/MEBeauty‑database )",,,,,
FBPFormer: Dynamic convolutional transformer for global-local-contexual facial beauty prediction,https://doi.org/10.1007/978-3-031-44204-9_19,Fábio,"FBPFormer (Deep, ViT + Dynamic Conv): base ViT‑B (12 Transformer blocks, d = 768, ImageNet pre‑train). In the last block the standard multi‑head self‑attention is replaced by a Dynamic‑Multi‑Head Self‑Attention (DMSA) module: Q/K/V are first passed through three dynamic 1‑D convolutions whose kernels are generated on‑the‑fly by a parameter generator conditioned on a pseudo facial‑attribute embedding produced by a lightweight CNN (7 × 7 + 5 × 5 convs, GAP). Best variant uses channel‑scaling k = 64 (1.75 M extra params). instance‑level Dynamic Exponential Loss (DELoss) to emphasise hard samples.",Facial beauty (non‑surgical aesthetics),"Objective continuous score regression (mean of 75 ratings, 1–5 scale)","Addresses the gap between CNNs (strong local features) and ViTs (strong global features) by injecting sample‑specific dynamic convolutions into ViT self‑attention, so global context is modulated by local, attribute‑like cues.Also created dynamic exponential loss to boost robustness to difficult faces. Experiments on SCUT‑FBP5500 show the model surpasses previous CNN (R³CNN, ResNet‑101) and Transformer (ViT‑B, Swin‑B, ConvNeXt‑B) baselines. ","Evaluated only on SCUT‑FBP5500 (Asian/Caucasian);
Relies on down‑sampled “pseudo attributes”; no explicit landmark guidance.
heavier compute than CNN baselines","5‑fold CV on SCUT‑FBP5500 (k = 64 DMSA, DELoss): PC = 0.9183, MAE = 0.2053, RMSE = 0.2734 (avg). Swin‑B: PC 0.9105 / MAE 0.2236 / RMSE 0.2944; R³CNN: PC 0.9142 / MAE 0.2130 / RMSE 0.2800. Ablations show (i) shrinking channel dim from full ViT to 1/64 still improves ViT‑B, (ii) placing the dynamic block in the final layer beats earlier positions. ",SCUT‑FBP5500 ( https://github.com/HCIILAB/SCUT‑FBP5500‑Database‑Release),,,,,
